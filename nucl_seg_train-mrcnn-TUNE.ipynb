{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e973065f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in /home/snk218/.local/lib/python3.9/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /ext3/miniconda3/lib/python3.9/site-packages (from seaborn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /ext3/miniconda3/lib/python3.9/site-packages (from seaborn) (1.22.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /ext3/miniconda3/lib/python3.9/site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /ext3/miniconda3/lib/python3.9/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (4.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /ext3/miniconda3/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /ext3/miniconda3/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /ext3/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ae05d3-5e80-4380-9017-0d0f4ea11ab0",
   "metadata": {
    "executionInfo": {
     "elapsed": 4215,
     "status": "ok",
     "timestamp": 1651070822140,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "46ae05d3-5e80-4380-9017-0d0f4ea11ab0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "from skimage import draw,io,segmentation\n",
    "import time\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import helper_functions as myutils\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3384c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n",
      "Device being used: cuda\n"
     ]
    }
   ],
   "source": [
    "device=myutils.set_to_gpu()\n",
    "myutils.setup_seed(4)\n",
    "ia.seed(5)\n",
    "myutils.gpu_mem_allocated()\n",
    "TRAIN_PATH=\"/scratch_tmp/snk218/stage1_train\"\n",
    "IMG_CHANNELS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bace58f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW IS TAKEN FROM 3rd place entry in competition: \"DeepRetina\" (https://github.com/Gelu74/DSB_2018)\n",
    "# \"The dataset doesn't have a standard train/val split, so I picked a variety of images to surve as a validation set.\"\n",
    "VAL_IMAGE_IDS = [\n",
    "    \"0c2550a23b8a0f29a7575de8c61690d3c31bc897dd5ba66caec201d201a278c2\",\n",
    "    \"92f31f591929a30e4309ab75185c96ff4314ce0a7ead2ed2c2171897ad1da0c7\",\n",
    "    \"1e488c42eb1a54a3e8412b1f12cde530f950f238d71078f2ede6a85a02168e1f\",\n",
    "    \"c901794d1a421d52e5734500c0a2a8ca84651fb93b19cec2f411855e70cae339\",\n",
    "    \"8e507d58f4c27cd2a82bee79fe27b069befd62a46fdaed20970a95a2ba819c7b\",\n",
    "    \"60cb718759bff13f81c4055a7679e81326f78b6a193a2d856546097c949b20ff\",\n",
    "    \"da5f98f2b8a64eee735a398de48ed42cd31bf17a6063db46a9e0783ac13cd844\",\n",
    "    \"9ebcfaf2322932d464f15b5662cae4d669b2d785b8299556d73fffcae8365d32\",\n",
    "    \"1b44d22643830cd4f23c9deadb0bd499fb392fb2cd9526d81547d93077d983df\",\n",
    "    \"97126a9791f0c1176e4563ad679a301dac27c59011f579e808bbd6e9f4cd1034\",\n",
    "    \"e81c758e1ca177b0942ecad62cf8d321ffc315376135bcbed3df932a6e5b40c0\",\n",
    "    \"f29fd9c52e04403cd2c7d43b6fe2479292e53b2f61969d25256d2d2aca7c6a81\",\n",
    "    \"0ea221716cf13710214dcd331a61cea48308c3940df1d28cfc7fd817c83714e1\",\n",
    "    \"3ab9cab6212fabd723a2c5a1949c2ded19980398b56e6080978e796f45cbbc90\",\n",
    "    \"ebc18868864ad075548cc1784f4f9a237bb98335f9645ee727dac8332a3e3716\",\n",
    "    \"bb61fc17daf8bdd4e16fdcf50137a8d7762bec486ede9249d92e511fcb693676\",\n",
    "    \"e1bcb583985325d0ef5f3ef52957d0371c96d4af767b13e48102bca9d5351a9b\",\n",
    "    \"947c0d94c8213ac7aaa41c4efc95d854246550298259cf1bb489654d0e969050\",\n",
    "    \"cbca32daaae36a872a11da4eaff65d1068ff3f154eedc9d3fc0c214a4e5d32bd\",\n",
    "    \"f4c4db3df4ff0de90f44b027fc2e28c16bf7e5c75ea75b0a9762bbb7ac86e7a3\",\n",
    "    \"4193474b2f1c72f735b13633b219d9cabdd43c21d9c2bb4dfc4809f104ba4c06\",\n",
    "    \"f73e37957c74f554be132986f38b6f1d75339f636dfe2b681a0cf3f88d2733af\",\n",
    "    \"a4c44fc5f5bf213e2be6091ccaed49d8bf039d78f6fbd9c4d7b7428cfcb2eda4\",\n",
    "    \"cab4875269f44a701c5e58190a1d2f6fcb577ea79d842522dcab20ccb39b7ad2\",\n",
    "    \"8ecdb93582b2d5270457b36651b62776256ade3aaa2d7432ae65c14f07432d49\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f9b996-d9b7-4e75-bffc-ec6dbd41aeb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1651071608440,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "33f9b996-d9b7-4e75-bffc-ec6dbd41aeb3"
   },
   "outputs": [],
   "source": [
    "# For training: resize image to atleast 512 at min. side and crop to 512x512\n",
    "MIN_DIM=512 # resize original image to atleast 512 at the minimum side\n",
    "CROP_DIM=512 # if larger, crop to 512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e314b12-62b6-47d3-b1bd-c3735f3fecde",
   "metadata": {
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1651071613682,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "5e314b12-62b6-47d3-b1bd-c3735f3fecde"
   },
   "outputs": [],
   "source": [
    "# Nucleus data set\n",
    "\n",
    "# pytorch Mask R CNN model expects an Image (PIL) and, for ground truth targets, a dictionary with these keys:\n",
    "# boxes (FloatTensor[N, 4]): the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
    "# labels (Int64Tensor[N]): the label for each bounding box\n",
    "# image_id (Int64Tensor[1]): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
    "# area (Tensor[N]): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
    "# iscrowd (UInt8Tensor[N]): instances with iscrowd=True will be ignored during evaluation.\n",
    "# (optionally) masks (UInt8Tensor[N, H, W]): The segmentation masks for each one of the objects\n",
    "# One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels.\n",
    "\n",
    "class NucleusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=True, min_dim=MIN_DIM, crop_dim=CROP_DIM, filter_ids=[], img_aug_seq=None):\n",
    "        self.root=root # e.g. \"stage1_train\"\n",
    "        self.train=train # will resize and random crop the image if True\n",
    "        self.resize_t=T.Resize(min_dim)\n",
    "        self.crop_dim=crop_dim\n",
    "        self.aug_seq=img_aug_seq\n",
    "\n",
    "        # a subset of the train set is being used as validation: \"filter_ids\": the image ids for validation \n",
    "        if(len(filter_ids)>0):\n",
    "            if(train):\n",
    "                img_ids=next(os.walk(root))[1]\n",
    "                self.img_ids=list(set(img_ids) - set(filter_ids))\n",
    "            else:\n",
    "                self.img_ids=filter_ids\n",
    "        else:\n",
    "            self.img_ids=next(os.walk(root))[1]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if(type(idx) == type(\"\")):  # Note: don't use this when sending to model on cuda device\n",
    "          image_id = idx\n",
    "          idx=self.img_ids.index(image_id)\n",
    "        else:\n",
    "          image_id = self.img_ids[idx] # Use numerical index when sending to model on cuda device\n",
    "          \n",
    "        # Load image\n",
    "        img_file = os.path.join(self.root, image_id, \"images\", image_id+\".png\")\n",
    "        image = Image.open(img_file).convert(\"RGB\") # must return PIL image \n",
    "        \n",
    "        # Load masks - masks are a series of binary images - one per nucleus\n",
    "        mask_files = next(os.walk(os.path.join(self.root, image_id, \"masks\")))[2]\n",
    "        masks = np.empty((len(mask_files),image.height,image.width), dtype=bool)\n",
    "        \n",
    "        for i,mask_file in enumerate(mask_files):\n",
    "            masks[i] = imread(os.path.join(self.root, image_id, \"masks\", mask_file)) != 0\n",
    "\n",
    "        if self.train and self.aug_seq is not None: \n",
    "            # Image augmentations\n",
    "            # convert mask array to SegmentationMapsOnImage instance: input array shape = (H,W,C)\n",
    "            segmap = SegmentationMapsOnImage(np.stack(masks,axis=-1), shape=np.array(image).shape)\n",
    "            image_aug, segmap_aug = self.aug_seq(image=np.array(image), segmentation_maps=segmap)\n",
    "            \n",
    "            masks=segmap_aug.get_arr()\n",
    "            masks=np.transpose(masks, (2, 0, 1))\n",
    "            \n",
    "            image=F.to_pil_image(image_aug,mode=\"RGB\")\n",
    "            \n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        if self.train: \n",
    "            ##### RESIZE/RANDOM CROPS #####\n",
    "            # resize image and masks\n",
    "            image=self.resize_t(image)\n",
    "            masks=self.resize_t(masks) \n",
    "            \n",
    "            # random crop image\n",
    "            w, h = image.size  \n",
    "            tw, th = self.crop_dim,self.crop_dim\n",
    "            \n",
    "            if (w > tw or h > th):\n",
    "                if(w <= tw): tw=w\n",
    "                if(h <= th): th=h\n",
    "                top = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "                left = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "                image = image.crop((left, top, left + tw, top + th))  \n",
    "\n",
    "                # crop masks\n",
    "                masks=masks[..., top:top + th, left:left + tw]\n",
    "\n",
    "        # Set up target dictionary return structure \n",
    "        # get bbox coords for each mask\n",
    "        boxes = []\n",
    "        rr = []\n",
    "        for i in range(len(masks)):\n",
    "            pos = np.where(masks[i])\n",
    "            if(len(pos[0]) > 0):\n",
    "                # filter if mask not in the image (since it was cropped)\n",
    "                # NOTE: the bbox expected is (x0,y0,x-max,y-max)\n",
    "                # NOT usual numpy way!\n",
    "                xmin = np.min(pos[1]) \n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "                \n",
    "                # filter if mask bbox length or width == 1\n",
    "                if(xmax-xmin > 0 and ymax-ymin > 0):\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    rr.append(i)\n",
    "                \n",
    "        num_objs = len(rr)\n",
    "\n",
    "        if(num_objs==0):\n",
    "            return None\n",
    "\n",
    "        masks = masks[rr,:]\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64) # there is only one class\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8) \n",
    "        idx=torch.as_tensor(idx,dtype=torch.int64)\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd: \"instances with iscrowd==True wil be ignored during evaluation\"\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes        # bounding boxes of the nuclei masks\n",
    "        target[\"labels\"] = labels      # labels, an array of ones, since we only have one class (nucleus)\n",
    "        target[\"masks\"] = masks        # pixel coordinates of the nuclei masks\n",
    "        target[\"image_id\"] = idx       # image id: numerical index\n",
    "        target[\"area\"] = area          # mask areas\n",
    "        target[\"iscrowd\"] = iscrowd    # always set to False \n",
    "\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "def nucleus_collate_fn(batch):\n",
    "    batch = filter(lambda x:x is not None, batch)\n",
    "    return tuple(zip(*batch))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83cdfb2f-1f71-4601-9d68-3d711afd664b",
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1651071619089,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "83cdfb2f-1f71-4601-9d68-3d711afd664b"
   },
   "outputs": [],
   "source": [
    "def show_data(img,label,model_output=False):\n",
    "\n",
    "    if(isinstance(img, torch.Tensor)):\n",
    "        img_=np.array(F.to_pil_image(img,mode=\"RGB\"))\n",
    "    else:\n",
    "        img_=img\n",
    "  \n",
    "    if(model_output):\n",
    "        fig,ax=plt.subplots(1,3,figsize=(20,30))\n",
    "    else:\n",
    "        #fig,ax=plt.subplots(1,2,figsize=(10,20))\n",
    "        fig,ax=plt.subplots(1,1,figsize=(20,20))\n",
    "\n",
    "    out_img=img_.copy()\n",
    "    \n",
    "    if(len(label['masks'])>0):\n",
    "        labeled_img=np.zeros(label['masks'][0].squeeze().detach().numpy().shape, dtype='uint8') \n",
    "        \n",
    "        if(model_output):\n",
    "            for i,bbox in enumerate(label[\"boxes\"]):\n",
    "                if(label['scores'][i]>=0.5):\n",
    "                    bbox_=[int(x.item()) for x in bbox]\n",
    "                    mask=(label['masks'][i].squeeze().detach().numpy()>0.5).astype('uint8')\n",
    "\n",
    "                    #print(f\"{i}: {bbox}\")\n",
    "                    \n",
    "\n",
    "                    if((i+1)<256): \n",
    "                        # check first if this mask is overlaping a previous mask: if so, skip it.\n",
    "                        # is this normal for MaskRCNN to output overlapping masks?\n",
    "                        if np.max(mask*labeled_img)>0:\n",
    "                            pass\n",
    "                            #print(\"Overlap\")\n",
    "                        else:\n",
    "                            labeled_img = labeled_img+(mask*(i+1))\n",
    "\n",
    "                            # Note!  bboxes are in x0,y0,xmax,ymax format \n",
    "                            rr,cc = draw.rectangle_perimeter((bbox_[1],bbox_[0]), \n",
    "                                                      extent=(bbox_[3]-bbox_[1],\n",
    "                                                              bbox_[2]-bbox_[0]),\n",
    "                                                      shape=out_img.shape)\n",
    "                    \n",
    "                            out_img[rr, cc, :] = [255,0,0]\n",
    "                        \n",
    "                    #if(i >= 2): break # ********* # <------\n",
    "                    \n",
    "                else:\n",
    "                    break # scores are sorted high to low\n",
    "            marked=segmentation.mark_boundaries(img_,labeled_img,) \n",
    "        else:\n",
    "            for i,mask in enumerate(label['masks']):\n",
    "                if((i+1)<256):\n",
    "                    labeled_img = labeled_img+(mask.detach().numpy()*(i+1))\n",
    "            marked=segmentation.mark_boundaries(img_,labeled_img,) \n",
    "\n",
    "            for bbox in label[\"boxes\"]:\n",
    "                bbox_=[int(x.item()) for x in bbox]\n",
    "\n",
    "                # Note!  bboxes are in x0,y0,xmax,ymax format \n",
    "                rr, cc = draw.rectangle_perimeter((bbox_[1],bbox_[0]), \n",
    "                                                  extent=(bbox_[3]-bbox_[1],\n",
    "                                                          bbox_[2]-bbox_[0]),\n",
    "                                                  shape=out_img.shape)\n",
    "                out_img[rr, cc, :] = [255,0,0]\n",
    "    else:\n",
    "        print(\"Alert: No masked nuclei in image.\")\n",
    "        marked=out_img\n",
    "        \n",
    "    if(model_output):\n",
    "        print(f\"Dim: h={img_.shape[0]} w={img_.shape[1]}\")\n",
    "        ax[0].imshow(out_img)\n",
    "        ax[1].imshow(labeled_img)\n",
    "        ax[2].imshow(marked)\n",
    "    else:\n",
    "        print(f\"Image ID: {label['image_id']}\") \n",
    "        print(f\"Dim: h={img_.shape[0]} w={img_.shape[1]}\")\n",
    "        #ax[0].imshow(out_img)\n",
    "        #ax[1].imshow(marked)\n",
    "        \n",
    "        ax.imshow(marked)\n",
    "    \n",
    "    #if(not model_output):\n",
    "    #    for bbox,area in zip(label[\"boxes\"],label['area']):\n",
    "    #        bbox_=[int(x.item()) for x in bbox]\n",
    "    #        ax[1].text(bbox_[0],bbox_[1],str(int(area.item())), color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985245a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet101\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops.feature_pyramid_network import LastLevelMaxPool\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "def resnet_with_fpn(backbone, trainable_backbone_layers):\n",
    "    \n",
    "    layers_to_train = [\"layer4\", \"layer3\", \"layer2\", \"layer1\", \"conv1\"][:trainable_backbone_layers]\n",
    "    if trainable_backbone_layers == 5:\n",
    "        layers_to_train.append(\"bn1\")\n",
    "\n",
    "    for name, parameter in backbone.named_parameters():\n",
    "        if all([not name.startswith(layer) for layer in layers_to_train]):\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "    extra_blocks = LastLevelMaxPool()\n",
    "    returned_layers = [1, 2, 3, 4]\n",
    "    return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
    "\n",
    "    in_channels_stage2 = backbone.inplanes // 8\n",
    "    in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
    "    out_channels = 256\n",
    "\n",
    "    return BackboneWithFPN(backbone, \n",
    "                             return_layers, \n",
    "                             in_channels_list, \n",
    "                             out_channels, \n",
    "                             extra_blocks=extra_blocks)\n",
    "\n",
    "def get_model_instance_segmentation_resnet101(pretrained, num_classes, max_detections_per_img):\n",
    "    \n",
    "    if(pretrained):\n",
    "        norm_layer=misc_nn_ops.FrozenBatchNorm2d\n",
    "        trainable_layers=3 # allow final 3 layers to be trained\n",
    "    else:\n",
    "        norm_layer=nn.BatchNorm2d\n",
    "        trainable_layers=5 # allow all layers to be trained\n",
    "        \n",
    "    backbone_resnet = resnet101(pretrained=pretrained, progress=True, norm_layer=norm_layer)\n",
    "    backbone=resnet_with_fpn(backbone_resnet, trainable_backbone_layers=trainable_layers) \n",
    "    \n",
    "    \n",
    "    \n",
    "    anchor_generator = AnchorGenerator(sizes=((8,), (16,), (32,), (64,), (128,)),  \n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),\n",
    "                                                     (0.5, 1.0, 2.0),\n",
    "                                                     (0.5, 1.0, 2.0),\n",
    "                                                     (0.5, 1.0, 2.0),\n",
    "                                                     (0.5, 1.0, 2.0)) )\n",
    "    \n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "    \n",
    "    mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n",
    "                                                     output_size=14,\n",
    "                                                     sampling_ratio=2)\n",
    "        \n",
    "    \n",
    "    model = MaskRCNN(backbone, \n",
    "                     num_classes=num_classes, \n",
    "                     rpn_anchor_generator=anchor_generator,\n",
    "                     box_roi_pool=roi_pooler,\n",
    "                     mask_roi_pool=mask_roi_pooler,\n",
    "                     box_detections_per_img=max_detections_per_img)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get number of input features for the box classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcda36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = get_model_instance_segmentation_resnet101(True, 2, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7945a1d0-6c24-4478-9ffa-0538a61ccb6d",
   "metadata": {
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1651071715255,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "7945a1d0-6c24-4478-9ffa-0538a61ccb6d"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model_instance_segmentation(pretrained, pretrained_backbone, num_classes, max_detections_per_img):\n",
    "    \n",
    "    # Allow to change below since there are more than 100 nuclei in some images and default is 100\n",
    "    # box_detections_per_img (int): maximum number of detections per image, for all classes \n",
    "\n",
    "    \n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=pretrained, \n",
    "                                                               pretrained_backbone=pretrained_backbone,\n",
    "                                                               box_detections_per_img=max_detections_per_img)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58831d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_model_instance_segmentation(pretrained=False, \n",
    "                                         pretrained_backbone=True, \n",
    "                                         num_classes=2, \n",
    "                                         max_detections_per_img=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86bd27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running model for detection, it can output overlapping masks\n",
    "# This not NOT allowed: Filter any masks that overlap another mask with a higher score\n",
    "# NOTE: this function also detach model from Gradient and cuda device, and convert to numpy\n",
    "def filter_output(model_output):\n",
    "    # filter any masks that overlap a mask higher in the list\n",
    "    # masks are sorted by score\n",
    "    # go through masks and remove any that overlap a mask higher in the list\n",
    "    \n",
    "    remove_idx=[]\n",
    "    labeled_img=np.zeros(model_output['masks'][0].squeeze().cpu().detach().numpy().shape, dtype='float32') \n",
    "    for i,mask in enumerate(model_output['masks']):\n",
    "        mask=(mask.squeeze().cpu().detach().numpy()>0.5).astype('float32')\n",
    "        if np.max(mask*labeled_img)>0:\n",
    "            remove_idx.append(i)\n",
    "        else:\n",
    "            labeled_img = labeled_img+(mask*(i+1))\n",
    "            \n",
    "    for key in model_output.keys():\n",
    "        model_output[key]=np.delete(model_output[key].cpu().detach().numpy(),remove_idx,axis=0)\n",
    "        \n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "BfZ87Htqw3I_",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1651071936517,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "BfZ87Htqw3I_"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pt.tar'): \n",
    "    # Note: Input model, optimizer and scheduler should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 0\n",
    "    best_model_epoch=0\n",
    "    best_meanAP=0\n",
    "\n",
    "    meanAP_arr = []\n",
    "    loss_arr = []\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        \n",
    "        loss_arr=checkpoint['loss_arr']\n",
    "        meanAP_arr=checkpoint['meanAP_arr']\n",
    "\n",
    "        best_meanAP = checkpoint['best_meanAP']\n",
    "        best_model_epoch = checkpoint['best_model_epoch']\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return (model, optimizer, scheduler, start_epoch, \n",
    "            meanAP_arr, loss_arr, best_model_epoch, best_meanAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pCOW2IyrxKYr",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1651071940756,
     "user": {
      "displayName": "Sarah Keegan",
      "userId": "10215270972531416907"
     },
     "user_tz": 240
    },
    "id": "pCOW2IyrxKYr"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, scheduler, num_epochs = 10, file_pre=\"model_state\"):\n",
    "\n",
    "    meanAP_arr = []\n",
    "    loss_arr = []\n",
    "\n",
    "    best_meanAP = 0\n",
    "    best_model_epoch=0\n",
    "    phases = ['train','validate']\n",
    "    since = time.time()\n",
    "    \n",
    "    for i in range(num_epochs): \n",
    "        \n",
    "        \n",
    "        (model,\n",
    "         optimizer, \n",
    "         scheduler, \n",
    "         i,\n",
    "         meanAP_arr, \n",
    "         loss_arr, \n",
    "         best_model_epoch,\n",
    "         best_meanAP) = load_checkpoint(model, \n",
    "                                        optimizer, \n",
    "                                        scheduler,\n",
    "                                        filename=f\"{file_pre}_checkpoint.pt.tar\")\n",
    "                            \n",
    "        if(i >= num_epochs):\n",
    "            break  # in case we've already run all epochs\n",
    "            \n",
    "        print('Epoch: {}/{}'.format(i, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        \n",
    "        for p in phases:\n",
    "            \n",
    "            running_total = 0\n",
    "\n",
    "            if p == 'train':\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "            else:\n",
    "                running_meanAP = 0\n",
    "                model.eval()\n",
    "            \n",
    "            # loop through batches:\n",
    "            #for n, (images,targets) in tqdm(enumerate(dataloader[p]), total=len(dataloader[p])):\n",
    "            for n, (images,targets) in enumerate(dataloader[p]):\n",
    "            \n",
    "                num_imgs=len(images)\n",
    "                images = list(image.to(device) for image in images)\n",
    "\n",
    "                if p == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                    loss = model(images, targets)\n",
    "                    \n",
    "                    losses = sum(loss for loss in loss.values())\n",
    "                    losses.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    #print(f\"losses={losses.item()}\")\n",
    "                    running_loss += losses.item()*num_imgs\n",
    "                else:\n",
    "                    print(myutils.gpu_mem_allocated())\n",
    "                    outputs = model(images)\n",
    "\n",
    "                    # Mean AP over IoU range from 0.5 to 0.95 for each image in batch\n",
    "                    # Average over all images in batch\n",
    "                    for img_i in range(len(images)):\n",
    "                        filtered_output=filter_output(outputs[img_i])\n",
    "                        \n",
    "                        #meanAP, all_AP, precisions, recalls=myutils.compute_ap_range(\n",
    "                        meanAP, all_AP, precisions, recalls=myutils.compute_ap_range(\n",
    "                                                        myutils.rearrange_boxes(targets[img_i]['boxes'].numpy()), \n",
    "                                                        targets[img_i]['labels'].numpy(), \n",
    "                                                        np.stack(targets[img_i]['masks'],axis=-1),\n",
    "                                                        myutils.rearrange_boxes(filtered_output['boxes']), \n",
    "                                                        filtered_output['labels'], \n",
    "                                                        filtered_output['scores'], \n",
    "                                                        np.stack(filtered_output['masks'].squeeze(),axis=-1),\n",
    "                                                        verbose=0)\n",
    "                        #print(f\"meanAP={meanAP}\")\n",
    "                        running_meanAP+=meanAP\n",
    "                        \n",
    "                    del outputs\n",
    "                    #print(myutils.gpu_mem_allocated())\n",
    "                        \n",
    "                running_total += num_imgs\n",
    "                \n",
    "                del images\n",
    "                del targets\n",
    "                #print(myutils.gpu_mem_allocated())\n",
    "                \n",
    "            #print(\"End loop dataloader\")\n",
    "            print(myutils.gpu_mem_allocated())\n",
    "\n",
    "            # Finish up for the epoch:\n",
    "            if p == 'train':\n",
    "                epoch_loss=float(running_loss/running_total)\n",
    "                \n",
    "                print('Phase: train, epoch loss: {:.6f}'.format(epoch_loss))\n",
    "                loss_arr.append(epoch_loss) # epoch training loss\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            else: #p == 'validate':\n",
    "                epoch_meanAP = float(running_meanAP/running_total)\n",
    "                \n",
    "                print('Phase: val, epoch meanAP: {:.6f}'.format(epoch_meanAP))\n",
    "                meanAP_arr.append(epoch_meanAP) # epoch validation meanAP\n",
    "\n",
    "                if epoch_meanAP > best_meanAP: \n",
    "                    best_meanAP = epoch_meanAP\n",
    "                    best_model_epoch=i\n",
    "                    \n",
    "        # SAVE STATE AT END OF EPOCH\n",
    "        checkpoint = {'epoch': i+1,\n",
    "             'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(),\n",
    "             'scheduler': scheduler.state_dict(),\n",
    "             'meanAP_arr': meanAP_arr,\n",
    "             'loss_arr': loss_arr,\n",
    "             'best_meanAP': best_meanAP,\n",
    "             'best_model_epoch': best_model_epoch,\n",
    "             } \n",
    "        torch.save(checkpoint, f\"{file_pre}_checkpoint.pt.tar\")\n",
    "        if(best_model_epoch == i):\n",
    "            torch.save(checkpoint, f\"{file_pre}_checkpoint-epoch_{i}.pt.tar\")\n",
    "        del checkpoint\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(f\"Best val epoch: {best_model_epoch}\")\n",
    "    print('Best val meanAP: {:6f}'.format(best_meanAP))\n",
    "    \n",
    "    checkpoint=torch.load(f\"{file_pre}_checkpoint-epoch_{best_model_epoch}.pt.tar\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model, best_model_epoch, best_meanAP, meanAP_arr, loss_arr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32773469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myutils.gpu_mem_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a00826a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learning rate: 1e-05\n",
      "Batch size: 4\n",
      "=> no checkpoint found at 'MRCNN_tune-lr_1e-05_bs_4_chk_checkpoint.pt.tar'\n",
      "Epoch: 0/2\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7538/2925540687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mbest_meanAP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mmeanAP_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mloss_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                                      \u001b[0mdata_loader_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                      \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7538/3131063171.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, scheduler, num_epochs, file_pre)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0;31m#print(f\"losses={losses.item()}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/miniconda3/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tune LR / Other hyperparameters?\n",
    "\n",
    "# Image augmentations\n",
    "seq = iaa.SomeOf((0, 4), [\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.OneOf([iaa.Affine(rotate=90),\n",
    "               iaa.Affine(rotate=180),\n",
    "               iaa.Affine(rotate=270)]),\n",
    "    iaa.Multiply((0.8, 1.5)),\n",
    "    iaa.GaussianBlur(sigma=(0.0, 5.0)),\n",
    "    iaa.OneOf([iaa.ScaleX((0.25, 2)),\n",
    "               iaa.ScaleY((0.25, 2))]),\n",
    "])\n",
    "\n",
    "tune_params=True\n",
    "if(tune_params):\n",
    "    dataset_dict = {\n",
    "    'train': NucleusDataset(TRAIN_PATH, train=True, filter_ids=VAL_IMAGE_IDS, img_aug_seq=seq), \n",
    "    'validate': NucleusDataset(TRAIN_PATH, train=False, filter_ids=VAL_IMAGE_IDS) \n",
    "    }\n",
    "\n",
    "    num_epochs=3\n",
    "    \n",
    "    learning_rates=[0.00001,0.0001,0.001,0.01]\n",
    "    batch_sizes = [4,6]\n",
    "    \n",
    "    all_best_losses=[]\n",
    "    all_best_meanAPs=[]\n",
    "    all_best_epochs=[]\n",
    "    \n",
    "    overall_best_meanAP=0\n",
    "    overall_best_lr=0\n",
    "    overall_best_bs=0\n",
    "\n",
    "    load_from_file=True\n",
    "    for learn_rate in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            print()\n",
    "            print(f\"Learning rate: {learn_rate}\")\n",
    "            print(f\"Batch size: {bs}\")\n",
    "\n",
    "            save_file=f\"MRCNN_tune-lr_{learn_rate}_bs_{bs}_best_model_state.pt\"\n",
    "\n",
    "            if(load_from_file and os.path.exists(save_file)):\n",
    "                state_dict=torch.load(save_file)\n",
    "            else:\n",
    "                data_loader_dict = {\n",
    "                    'train':torch.utils.data.DataLoader(dataset_dict['train'],\n",
    "                                        batch_size=bs, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        collate_fn=nucleus_collate_fn),\n",
    "                    \n",
    "                    'validate':torch.utils.data.DataLoader(dataset_dict['validate'],\n",
    "                                        batch_size=bs, \n",
    "                                        shuffle=False, \n",
    "                                        num_workers=4,\n",
    "                                        collate_fn=nucleus_collate_fn)\n",
    "                }\n",
    "                \n",
    "                \n",
    "\n",
    "                #model = get_model_instance_segmentation_resnet101(True, 2, 500) \n",
    "\n",
    "                model = get_model_instance_segmentation(pretrained=True, \n",
    "                                                         pretrained_backbone=False, \n",
    "                                                         num_classes=2, \n",
    "                                                         max_detections_per_img=500) \n",
    "\n",
    "                # move model to the right device\n",
    "                model.to(device)\n",
    "\n",
    "                params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "                optimizer = torch.optim.Adam(params, lr=learn_rate) \n",
    "                lambda_func = lambda epoch: 0.5 ** epoch \n",
    "                scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)\n",
    "\n",
    "                (best_model,\n",
    "                best_model_epoch, \n",
    "                best_meanAP, \n",
    "                meanAP_arr, \n",
    "                loss_arr)=train_model(model, \n",
    "                                     data_loader_dict, \n",
    "                                     optimizer, \n",
    "                                     scheduler, \n",
    "                                     num_epochs,  \n",
    "                                     file_pre=f\"MRCNN_tune-lr_{learn_rate}_bs_{bs}_chk\")\n",
    "    \n",
    "                best_model_state = {\n",
    "                         'best_model': best_model.state_dict(),\n",
    "                         'best_model_epoch': best_model_epoch,\n",
    "                         'best_meanAP': best_meanAP,\n",
    "                         'meanAP_arr': meanAP_arr,\n",
    "                         'loss_arr': loss_arr,\n",
    "                         } \n",
    "                torch.save(best_model_state, f\"MRCNN_tune-lr_{learn_rate}_bs_{bs}_best_model_state.pt\")\n",
    "   \n",
    "            cur_best_meanAP = best_model_state['meanAP_arr'][best_model_state['best_model_epoch']]\n",
    "            cur_best_loss = best_model_state['loss_arr'][best_model_state['best_model_epoch']]\n",
    "\n",
    "            all_best_losses.append(cur_best_loss)\n",
    "            all_best_meanAPs.append(cur_best_meanAP)\n",
    "            all_best_epochs.append(best_model_state['best_model_epoch'])\n",
    "            if(cur_best_meanAP > overall_best_meanAP):\n",
    "                overall_best_meanAP = cur_best_meanAP\n",
    "                overall_best_bs=bs\n",
    "                overall_best_lr=learn_rate\n",
    "\n",
    "    print(overall_best_bs)\n",
    "    print(overall_best_lr)\n",
    "    print(overall_best_loss)\n",
    "    print(cur_best_meanAP)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e11ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "nucl_seg_train.ipynb",
   "provenance": [
    {
     "file_id": "1Ai8mcyM9MRp43UwGgU8kW582crrJvq8l",
     "timestamp": 1650654753411
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13e50995b6844b11baf5340a2d43c176": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6e0408d13ad4066b999dd74b19b74b1",
      "max": 178090079,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9638e60ed354495a94ff91b40d5cc52",
      "value": 178090079
     }
    },
    "2415d32327064fd5ba6c99121558d542": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77315c676cc347ecb66661301f869fab",
      "placeholder": "​",
      "style": "IPY_MODEL_ebd3c3ea5a294ed3b14b4526d2d14096",
      "value": " 170M/170M [00:02&lt;00:00, 70.8MB/s]"
     }
    },
    "4a564dfc115849819b2739ef992acd04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51c3146cffb9496a9c2c379b07d7a274": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8de344c72ca549bbb1920e319a4c4836",
      "placeholder": "​",
      "style": "IPY_MODEL_c3e1ddbd050f48aa92fef7dbeafb47c0",
      "value": "100%"
     }
    },
    "77315c676cc347ecb66661301f869fab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8de344c72ca549bbb1920e319a4c4836": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9638e60ed354495a94ff91b40d5cc52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3e1ddbd050f48aa92fef7dbeafb47c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6e0408d13ad4066b999dd74b19b74b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8248c2b43ea4056bff0a3401358c71f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51c3146cffb9496a9c2c379b07d7a274",
       "IPY_MODEL_13e50995b6844b11baf5340a2d43c176",
       "IPY_MODEL_2415d32327064fd5ba6c99121558d542"
      ],
      "layout": "IPY_MODEL_4a564dfc115849819b2739ef992acd04"
     }
    },
    "ebd3c3ea5a294ed3b14b4526d2d14096": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
